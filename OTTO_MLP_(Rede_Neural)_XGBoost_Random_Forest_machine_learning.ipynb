{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explicações sobre o código:**\n",
        "______________________________________________________________________\n",
        "Dataset Fictício: O código cria um conjunto de dados fictício com informações sobre colaboradores e suas habilidades em inglês.\n",
        "Pré-processamento: Utiliza o StandardScaler para normalizar os dados e prepara as variáveis para o treinamento.\n",
        "\n",
        "______________________________________________________________________\n",
        "Modelos Preditivos:\n",
        "Random Forest e XGBoost são usados como classificadores base para prever a aprovação/feedback de cada aluno.\n",
        "\n",
        "______________________________________________________________________\n",
        "\n",
        "MLP (Rede Neural) é utilizada para explorar o poder das redes neurais profundas.\n",
        "Algoritmo OTTO: Usado para recomendação de conteúdo baseado na decomposição das habilidades dos alunos (via NMF).\n",
        "\n",
        "______________________________________________________________________\n",
        "\n",
        "Feedback em Tempo Real: A função feedback_em_tempo_real gera sugestões imediatas para alunos com base nas previsões feitas pelos modelos de classificação.\n",
        "\n",
        "\n",
        "______________________________________________________________________\n",
        "Otimização de Hiperparâmetros: Utiliza o GridSearchCV para melhorar os parâmetros de modelos como o XGBoost.\n",
        "______________________________________________________________________\n",
        "Visualizações: O código inclui visualizações de correlação, distribuição de feedbacks e a importância das features nos modelos.\n",
        "______________________________________________________________________\n",
        "\n",
        "Esse código integra tudo que discutimos, com a estrutura robusta necessária para desenvolver um sistema de previsão e recomendação eficaz para o ensino de inglês. Ele pode ser facilmente expandido para incluir mais alunos, mais características ou até novos modelo"
      ],
      "metadata": {
        "id": "LdIkUkrxeFM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "FW82aMSidbbB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# 1. Criando o dataset fictício\n",
        "np.random.seed(42)\n",
        "n = 500\n",
        "data = {\n",
        "    'colaborador_id': range(1, n+1),\n",
        "    'vocabulário': np.random.randint(1, 11, size=n),\n",
        "    'gramática': np.random.randint(1, 11, size=n),\n",
        "    'fluência': np.random.randint(1, 11, size=n),\n",
        "    'pronúncia': np.random.randint(1, 11, size=n),\n",
        "    'audição': np.random.randint(1, 11, size=n),\n",
        "    'escrita': np.random.randint(1, 11, size=n),\n",
        "    'leitura': np.random.randint(1, 11, size=n),\n",
        "    'progresso': np.random.randint(1, 11, size=n),\n",
        "    'horas_estudo': np.random.randint(1, 20, size=n),\n",
        "    'interacao_com_conteudo': np.random.uniform(0, 1, n),\n",
        "    'tempo_resposta': np.random.uniform(1, 10, n)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "9kG0uY7ieE1T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szrjClnJaUi0",
        "outputId": "13e87411-939c-476f-850b-5ae8ebba7d63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EnRqah6k61fI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEK_B6hhiMVv",
        "outputId": "f7da65c0-e379-4ebc-81b4-b37d23cbc800"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['colaborador_id', 'vocabulário', 'gramática', 'fluência', 'pronúncia',\n",
            "       'audição', 'escrita', 'leitura', 'progresso', 'horas_estudo',\n",
            "       'interacao_com_conteudo', 'tempo_resposta'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()\n"
      ],
      "metadata": {
        "id": "p4rJ72mniPa8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caso tenha espaços extras ou digitação errada\n",
        "df.columns = df.columns.str.strip()  # Remove espaços extras\n"
      ],
      "metadata": {
        "id": "S4-wjnEuiVSn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentando acessar a coluna 'leitura'\n",
        "if 'leitura' in df.columns:\n",
        "    print(\"A coluna 'leitura' foi encontrada!\")\n",
        "else:\n",
        "    print(\"A coluna 'leitura' não foi encontrada.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ2OJ4VPipds",
        "outputId": "8add7244-b739-4e90-a1b7-6a7828610b7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A coluna 'leitura' foi encontrada!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibindo as primeiras linhas do DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dduaiWyit4N",
        "outputId": "13314695-4cb1-4025-fab8-bfd4f60e5443"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   colaborador_id  vocabulário  gramática  fluência  pronúncia  audição  \\\n",
            "0               1            7          9         1          6       10   \n",
            "1               2            4          1         8          4        6   \n",
            "2               3            8          1         4          4        7   \n",
            "3               4            5          4         4          4        9   \n",
            "4               5            7          9         5          2        1   \n",
            "\n",
            "   escrita  leitura  progresso  horas_estudo  interacao_com_conteudo  \\\n",
            "0        5        8          2            18                0.131654   \n",
            "1        4        9          6            11                0.854703   \n",
            "2        1        6          2             5                0.165302   \n",
            "3        5        5         10            15                0.854261   \n",
            "4       10        6          5            18                0.490765   \n",
            "\n",
            "   tempo_resposta  \n",
            "0        7.997099  \n",
            "1        7.055624  \n",
            "2        7.518291  \n",
            "3        5.433391  \n",
            "4        6.700706  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir todas as colunas do DataFrame\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlbaOG0ti68t",
        "outputId": "ccefb1fe-c892-4dba-c652-1e875947c26c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['colaborador_id', 'vocabulário', 'gramática', 'fluência', 'pronúncia', 'audição', 'escrita', 'leitura', 'progresso', 'horas_estudo', 'interacao_com_conteudo', 'tempo_resposta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo as features corretamente\n",
        "features = ['vocabulário', 'gramática', 'fluência', 'pronúncia',\n",
        "            'audição', 'escrita', 'leitura', 'progresso',\n",
        "            'horas_estudo', 'interacao_com_conteudo', 'tempo_resposta']\n",
        "\n",
        "# Acessando as features do DataFrame\n",
        "X = df[features]\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Verificando a normalização\n",
        "print(\"Primeiros valores normalizados (MinMaxScaler):\", X_scaled[:5])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdIPafG_iaIO",
        "outputId": "4e0e7720-3706-413f-97ad-02f831cd06fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeiros valores normalizados (MinMaxScaler): [[0.66666667 0.88888889 0.         0.55555556 1.         0.44444444\n",
            "  0.77777778 0.11111111 0.94444444 0.13079432 0.78009356]\n",
            " [0.33333333 0.         0.77777778 0.33333333 0.55555556 0.33333333\n",
            "  0.88888889 0.55555556 0.55555556 0.85654649 0.67510507]\n",
            " [0.77777778 0.         0.33333333 0.33333333 0.66666667 0.\n",
            "  0.55555556 0.11111111 0.22222222 0.16456878 0.72669928]\n",
            " [0.44444444 0.33333333 0.33333333 0.33333333 0.88888889 0.44444444\n",
            "  0.44444444 1.         0.77777778 0.85610217 0.49420192]\n",
            " [0.66666667 0.88888889 0.44444444 0.11111111 0.         1.\n",
            "  0.55555556 0.44444444 0.94444444 0.49124787 0.63552637]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Pré-processamento com MinMaxScaler\n",
        "features = ['vocabulário', 'gramática', 'fluência', 'pronúncia',\n",
        "            'audição', 'escrita', 'leitura', 'progresso',\n",
        "            'horas_estudo', 'interacao_com_conteudo', 'tempo_resposta']\n",
        "X = df[features]\n",
        "scaler = MinMaxScaler()  # Normaliza os dados para [0, 1]\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Verifique se X_scaled está no intervalo [0, 1]\n",
        "print(\"Primeiros valores normalizados (MinMaxScaler):\", X_scaled[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fSXyDaWfBVa",
        "outputId": "4ab9d587-58e7-4dfb-dac8-4663ff64bd95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeiros valores normalizados (MinMaxScaler): [[0.66666667 0.88888889 0.         0.55555556 1.         0.44444444\n",
            "  0.77777778 0.11111111 0.94444444 0.13079432 0.78009356]\n",
            " [0.33333333 0.         0.77777778 0.33333333 0.55555556 0.33333333\n",
            "  0.88888889 0.55555556 0.55555556 0.85654649 0.67510507]\n",
            " [0.77777778 0.         0.33333333 0.33333333 0.66666667 0.\n",
            "  0.55555556 0.11111111 0.22222222 0.16456878 0.72669928]\n",
            " [0.44444444 0.33333333 0.33333333 0.33333333 0.88888889 0.44444444\n",
            "  0.44444444 1.         0.77777778 0.85610217 0.49420192]\n",
            " [0.66666667 0.88888889 0.44444444 0.11111111 0.         1.\n",
            "  0.55555556 0.44444444 0.94444444 0.49124787 0.63552637]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definindo o OTTO\n",
        "class OTTORecommender:\n",
        "    def __init__(self, X, n_components=5):\n",
        "        self.X = X\n",
        "        self.n_components = n_components\n",
        "        self.W = None\n",
        "        self.H = None\n",
        "\n",
        "    def fit(self):\n",
        "        nmf_model = NMF(n_components=self.n_components, init='random', random_state=0)\n",
        "        self.W = nmf_model.fit_transform(self.X)\n",
        "        self.H = nmf_model.components_\n",
        "        print(\"OTTO treinado com sucesso!\")\n",
        "\n",
        "    def recommend(self, user_index, top_n=5):\n",
        "        user_profile = self.W[user_index, :]\n",
        "        similarities = pairwise_distances(self.H.T, user_profile.reshape(1, -1), metric='cosine')\n",
        "        recommended_items = np.argsort(similarities.flatten())\n",
        "        return recommended_items[:top_n]\n"
      ],
      "metadata": {
        "id": "KbEMQBSlfBXW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Treinando o OTTO\n",
        "otto = OTTORecommender(X_scaled, n_components=5)\n",
        "otto.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HoIxA1Z7IJ0",
        "outputId": "9860eb06-b393-4147-a063-4e3fd42e8a6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OTTO treinado com sucesso!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Recomendando para um aluno\n",
        "aluno_id = 15\n",
        "user_index = aluno_id - 1  # Ajustando para índice 0\n",
        "recomendacoes = otto.recommend(user_index=user_index, top_n=5)\n",
        "print(f\"Recomendações para o aluno {aluno_id}: {recomendacoes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyCkxwk07MBX",
        "outputId": "18ed9940-5443-4b3b-9a3d-b9509dd95ec3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recomendações para o aluno 15: [3 4 8 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Dados para o gráfico\n",
        "indices = [3, 4, 8, 2, 0]\n",
        "conteudos = [\"Pronúncia\", \"Audição\", \"Horas de Estudo\", \"Fluência\", \"Vocabulário\"]\n",
        "importancias = [0.25, 0.20, 0.15, 0.25, 0.15]  # Exemplo de pesos de importância\n",
        "\n",
        "# Criando o gráfico de pizza\n",
        "fig = go.Figure(\n",
        "    data=[go.Pie(\n",
        "        labels=conteudos,\n",
        "        values=importancias,\n",
        "        hoverinfo=\"label+percent+value\",\n",
        "        textinfo=\"label+percent\",\n",
        "        pull=[0.1 if i == 3 else 0 for i in range(len(indices))],  # Destaque no \"Pronúncia\"\n",
        "        marker=dict(colors=[\"#636EFA\", \"#EF553B\", \"#00CC96\", \"#AB63FA\", \"#FFA15A\"])\n",
        "    )]\n",
        ")\n",
        "\n",
        "# Configurações do layout\n",
        "fig.update_layout(\n",
        "    title=\"Distribuição de Recomendações para o Aluno 15\",\n",
        "    annotations=[dict(\n",
        "        text=\"Conteúdos\",\n",
        "        x=0.5, y=0.5,\n",
        "        font_size=15,\n",
        "        showarrow=False\n",
        "    )]\n",
        ")\n",
        "\n",
        "# Exibindo o gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "XZ2lhCPT8l77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# Dados das recomendações\n",
        "conteudos = [\"Pronúncia\", \"Audição\", \"Horas de Estudo\", \"Fluência\", \"Vocabulário\"]\n",
        "importancias = [25, 20, 15, 25, 15]  # Pesos de cada conteúdo\n",
        "\n",
        "# Criar DataFrame\n",
        "df_recomendacoes = pd.DataFrame({\n",
        "    \"Conteúdo\": conteudos,\n",
        "    \"Importância (%)\": importancias\n",
        "})\n",
        "\n",
        "# Criar gráfico de barras horizontal\n",
        "fig = px.bar(\n",
        "    df_recomendacoes,\n",
        "    x=\"Importância (%)\",\n",
        "    y=\"Conteúdo\",\n",
        "    orientation='h',\n",
        "    text=\"Importância (%)\",\n",
        "    title=\"Recomendações Personalizadas para o Aluno 15\",\n",
        "    labels={\"Importância (%)\": \"Peso da Importância (%)\", \"Conteúdo\": \"Áreas de Aprendizado\"},\n",
        "    color=\"Importância (%)\",\n",
        "    color_continuous_scale=['#FFFFFF', '#FF6F61', '#005BBB']  # Branco, vermelho suave e azul\n",
        ")\n",
        "\n",
        "# Configurar o layout com fundo personalizado\n",
        "fig.update_layout(\n",
        "    title_font_size=20,\n",
        "    xaxis_title=\"Peso da Importância (%)\",\n",
        "    yaxis_title=\"Áreas de Aprendizado\",\n",
        "    coloraxis_showscale=False,  # Remove a escala de cores\n",
        "    paper_bgcolor=\"white\",  # Fundo branco\n",
        "    plot_bgcolor=\"rgba(0,0,0,0)\",  # Fundo do gráfico transparente\n",
        "    images=[  # Adicionando a bandeira dos EUA como fundo\n",
        "        dict(\n",
        "            source=\"https://upload.wikimedia.org/wikipedia/en/a/a4/Flag_of_the_United_States.svg\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0, y=1.1,  # Ajuste da posição\n",
        "            sizex=1.8, sizey=1.5,\n",
        "            xanchor=\"left\", yanchor=\"top\",\n",
        "            opacity=0.3,  # Transparência do fundo\n",
        "            layer=\"below\"\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configurações adicionais das barras\n",
        "fig.update_traces(\n",
        "    texttemplate='%{text}%',\n",
        "    textposition='outside',\n",
        "    marker=dict(line=dict(color='black', width=1))  # Bordas para clareza\n",
        ")\n",
        "\n",
        "# Adiciona anotação para o conteúdo mais relevante\n",
        "fig.add_annotation(\n",
        "    x=25,\n",
        "    y=\"Pronúncia\",\n",
        "    text=\"Conteúdo Prioritário\",\n",
        "    showarrow=True,\n",
        "    arrowhead=2,\n",
        "    ax=50,\n",
        "    ay=-30,\n",
        "    font=dict(color=\"red\", size=14),\n",
        "    arrowcolor=\"red\"\n",
        ")\n",
        "\n",
        "# Exibir gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "rgwQtGNq-5LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Carregar o dataset Iris\n",
        "data = load_iris()\n",
        "\n",
        "# Definir X (dados de entrada) e y (rótulos de saída)\n",
        "X = data.data  # As variáveis independentes (atributos)\n",
        "y = data.target  # A variável alvo (rótulos)\n",
        "\n",
        "# Dividir os dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinando o modelo Random Forest\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Avaliação do modelo Random Forest\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f'Acurácia do Random Forest: {rf_accuracy:.2f}')\n",
        "print(\"Matriz de Confusão (Random Forest):\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Relatório de Classificação (Random Forest):\")\n",
        "print(classification_report(y_test, rf_predictions))\n"
      ],
      "metadata": {
        "id": "6qxKkn-lcAcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Para classificação multiclasse, você precisa calcular para cada classe\n",
        "y_score = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Calculando a Curva ROC e AUC para cada classe\n",
        "fpr, tpr, _ = roc_curve(y_test, y_score[:, 0], pos_label=0)  # Para a classe 0\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotando a curva ROC\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YRpOBSq7ca6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_test, rf_predictions)\n",
        "\n",
        "# Normalizando\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plotando a matriz de confusão normalizada\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap=\"Blues\", xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
        "plt.xlabel('Previsões')\n",
        "plt.ylabel('Valores Reais')\n",
        "plt.title('Matriz de Confusão Normalizada')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_D4iaQmWchOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Supondo que X_train e y_train são os dados de treinamento\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões\n",
        "rf_predictions = rf_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "eeqP9jsrb8AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suponha que X seja as variáveis independentes e y seja o alvo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "B1Szumg3b4Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo Random Forest\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f'Acurácia do Random Forest: {rf_accuracy:.2f}')\n",
        "print(\"Matriz de Confusão (Random Forest):\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Relatório de Classificação (Random Forest):\")\n",
        "print(classification_report(y_test, rf_predictions))"
      ],
      "metadata": {
        "id": "LMstMzmAfBbB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "N_SvD6FHjau4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost --upgrade\n"
      ],
      "metadata": {
        "id": "qUaQ8reLjjVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando o XGBClassifier da forma correta\n",
        "xgb_model = XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_predictions = xgb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "2o8Id2D6jmcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Modelagem preditiva com XGBoost\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_predictions = xgb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "FWdk2h9FfBdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo X (features) e y (target)\n",
        "features = ['vocabulário', 'gramática', 'fluência', 'pronúncia', 'audição', 'escrita', 'leitura', 'progresso', 'horas_estudo', 'interacao_com_conteudo', 'tempo_resposta']\n",
        "X = df[features]\n",
        "y = df['leitura']\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5OOOaZRhj8Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisão dos dados em treino e teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-gI_lsz1m4Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizando os dados\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "UYJfOrGZm6bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_adjusted = y_train - 1  # Subtrai 1 de todos os valores de y_train\n",
        "\n",
        "# Agora, treine o modelo XGBoost com os rótulos ajustados\n",
        "xgb_model.fit(X_train_scaled, y_train_adjusted)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xhz9Qtaxm9M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classes de y_train:\", y_train.unique())\n"
      ],
      "metadata": {
        "id": "o3pWhLinnOsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reindexando as classes\n",
        "y_train = y_train - 1\n",
        "y_test = y_test - 1\n"
      ],
      "metadata": {
        "id": "zZHFqgFMrTyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Valores únicos de y_train:\", np.unique(y_train))\n"
      ],
      "metadata": {
        "id": "xbrmeHPbrVKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Definindo os parâmetros para testar\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Usando GridSearchCV para encontrar os melhores parâmetros\n",
        "xgb = XGBClassifier(use_label_encoder=False)\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Exibindo os melhores parâmetros encontrados\n",
        "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
        "\n",
        "# Treinando o modelo com os melhores parâmetros\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "best_xgb_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "PfWVq1SGdByE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Ajuste de scale_pos_weight\n",
        "xgb_model = XGBClassifier(scale_pos_weight= (sum(y_train == 0) / sum(y_train == 1)), use_label_encoder=False)\n",
        "xgb_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZHyxSz9EdaEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Aplicando SMOTE para balancear as classes\n",
        "smote = SMOTE()\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Treinando o modelo com dados balanceados\n",
        "xgb_model = XGBClassifier(use_label_encoder=False)\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MvGAqIiTeHVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(learning_rate=0.05, use_label_encoder=False)\n",
        "xgb_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Na9ytIx5eOPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Avaliando o modelo com validação cruzada\n",
        "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)  # 'cv=5' divide em 5 subconjuntos\n",
        "print(f\"Acurácia média da validação cruzada: {cv_scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "id": "T34FVHeAeUUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Algoritmo OTTO para recomendação multiobjetivo\n",
        "class OTTORecommender:\n",
        "    def _init_(self, X, n_components=5):\n",
        "        self.X = X  # Matriz de características (desempenho dos alunos)\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def fit(self):\n",
        "        # Decomposição não negativa para encontrar os fatores subjacentes\n",
        "        model = NMF(n_components=self.n_components, init='random', random_state=0)\n",
        "        self.W = model.fit_transform(self.X)  # Componentes dos alunos\n",
        "        self.H = model.components_  # Componentes do conteúdo (conteúdo recomendado)\n",
        "\n",
        "    def recommend(self, user_index):\n",
        "        # Calculando as distâncias para recomendar conteúdo baseado nos fatores\n",
        "        user_profile = self.W[user_index, :]\n",
        "        similarities = pairwise_distances(self.H.T, user_profile.reshape(1, -1), metric='cosine')\n",
        "\n",
        "        # Recomendando o conteúdo mais próximo (de acordo com a similaridade)\n",
        "        recommended_items = np.argsort(similarities.flatten())\n",
        "        return recommended_items[:5]  # Retorna as top 5 recomendações"
      ],
      "metadata": {
        "id": "7LnNqcwNfBp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OTTORecommender:\n",
        "    def __init__(self, X_scaled):  # Agora aceita X_scaled como argumento\n",
        "        self.X_scaled = X_scaled\n",
        "        # Outros atributos podem ser inicializados aqui\n"
      ],
      "metadata": {
        "id": "jjz7QnBifBrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "OxamIU9ie9bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features)\n"
      ],
      "metadata": {
        "id": "Y1d9q-Fve-nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando as colunas no DataFrame\n",
        "print(f\"Colunas no DataFrame: {df.columns}\")\n",
        "\n",
        "# Verificando as features que estamos tentando acessar\n",
        "print(f\"Features: {features}\")\n",
        "\n",
        "# Verificando se a coluna 'leitura' está presente no DataFrame\n",
        "if 'leitura' not in df.columns:\n",
        "    print(\"A coluna 'leitura' não existe no DataFrame.\")\n",
        "else:\n",
        "    print(\"A coluna 'leitura' foi encontrada no DataFrame.\")\n",
        "\n",
        "# Verificando se o aluno existe no DataFrame\n",
        "if aluno_id not in df['colaborador_id'].values:\n",
        "    print(f\"Aluno {aluno_id} não encontrado no DataFrame.\")\n",
        "else:\n",
        "    print(f\"Aluno {aluno_id} encontrado.\")\n",
        "\n",
        "    # Recuperando os dados do aluno\n",
        "    try:\n",
        "        dados_aluno = df.loc[df['colaborador_id'] == aluno_id, features].values[0]\n",
        "        print(f\"Dados do aluno {aluno_id}: {dados_aluno}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Erro ao acessar as colunas do aluno: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EaWeahs4fEyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n"
      ],
      "metadata": {
        "id": "5dIR8P7G5S8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Criando o dataset fictício\n",
        "np.random.seed(42)\n",
        "n = 500\n",
        "data = {\n",
        "    'colaborador_id': range(1, n+1),\n",
        "    'vocabulário': np.random.randint(1, 11, size=n),\n",
        "    'gramática': np.random.randint(1, 11, size=n),\n",
        "    'fluência': np.random.randint(1, 11, size=n),\n",
        "    'pronúncia': np.random.randint(1, 11, size=n),\n",
        "    'audição': np.random.randint(1, 11, size=n),\n",
        "    'escrita': np.random.randint(1, 11, size=n),\n",
        "    'leitura': np.random.randint(1, 11, size=n),\n",
        "    'progresso': np.random.randint(1, 11, size=n),  # Variável alvo\n",
        "    'horas_estudo': np.random.randint(1, 20, size=n),\n",
        "    'interacao_com_conteudo': np.random.uniform(0, 1, n),\n",
        "    'tempo_resposta': np.random.uniform(1, 10, n)\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "FecdoNwB9B8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Definindo a variável alvo e variáveis independentes\n",
        "X = df.drop(columns=['colaborador_id', 'progresso'])  # Remover 'colaborador_id' e a variável alvo\n",
        "y = df['progresso']"
      ],
      "metadata": {
        "id": "8XOAF6kgKmxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Transformando a variável alvo em categorias (para classificação)\n",
        "# Aqui, podemos transformar 'progresso' em categorias. Por exemplo, de 1-3 -> baixa, 4-7 -> média, 8-10 -> alta\n",
        "y = pd.cut(y, bins=[0, 3, 7, 10], labels=['baixa', 'média', 'alta'])"
      ],
      "metadata": {
        "id": "XDY4yLNdKm68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Dividindo os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aKuR5XloKm9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Aplicando SMOTE para balanceamento de classes (se necessário)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "rtH-NP7IKnAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando a distribuição das classes\n",
        "print(\"Distribuição original das classes em y_train:\", y_train.value_counts())\n",
        "print(\"Distribuição após SMOTE em y_train_resampled:\", y_train_resampled.value_counts())"
      ],
      "metadata": {
        "id": "syevv6ibKnCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Escalonando os dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "YFaMy8wBKnEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Definindo o modelo e GridSearchCV para ajuste de hiperparâmetros\n",
        "mlp_model = MLPClassifier(max_iter=500)\n",
        "\n",
        "# Atualizando a definição do grid de hiperparâmetros sem o class_weight\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(100,), (100, 50), (200, 100)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'early_stopping': [True, False],  # Adicionando early stopping\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(mlp_model, param_grid, cv=3, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train_resampled)"
      ],
      "metadata": {
        "id": "wmXGpaOiK_Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Avaliando o modelo\n",
        "mlp_predictions = grid_search.best_estimator_.predict(X_test_scaled)\n",
        "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
        "print(f'Acurácia do MLP: {mlp_accuracy:.2f}')\n",
        "print(confusion_matrix(y_test, mlp_predictions))\n",
        "print(classification_report(y_test, mlp_predictions))"
      ],
      "metadata": {
        "id": "9JGT0xatK_O5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}